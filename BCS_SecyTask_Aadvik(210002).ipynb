{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BCS-SecyTask-Aadvik(210002)",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "MfIVXL8F1TYH"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from torchvision import datasets,transforms\n",
        "#from torchvision.transforms import ToTensor\n",
        "from torch import nn ,optim\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt \n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "KxikMW0U8Aqb"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(5, 5))\n",
        "cols, rows = 2 , 2\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(trainset), size=(1,)).item()\n",
        "    img, label = trainset[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(label)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "pLSRtP8lFUAG",
        "outputId": "bfcc1d23-f8a3-4516-9ad7-110c61a8e0d2"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module): \n",
        "      def __init__(self):\n",
        "          super().__init__()\n",
        "          self.i = nn.Linear(784,256)\n",
        "          self.h = nn.Linear(256,64)\n",
        "          self.output = nn.Linear(64,10)\n",
        "      def forward(self, x):\n",
        "          x = x.view(x.shape[0],-1)\n",
        "          x = torch.sigmoid(self.i(x))\n",
        "          x = torch.sigmoid(self.h(x))\n",
        "          x = F.softmax(self.output(x), dim=1)\n",
        "\n",
        "          return x"
      ],
      "metadata": {
        "id": "LaeQ6r1282cp"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Classifier()\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n"
      ],
      "metadata": {
        "id": "iT5VZ9T23Y6T"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 5\n",
        "iter = 0 \n",
        "for e in range (epoch):   \n",
        "    for images, labels in trainloader:\n",
        "       # images = images.reshape(-1)\n",
        "        out = model(images)\n",
        "        loss = criterion(out,labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        iter +=1\n",
        "\n",
        "        if iter%1000==0:\n",
        "          correct = 0\n",
        "          total = 0 \n",
        "          for images , labels in testloader:\n",
        "            labels = labels\n",
        "            outputs = model(images)\n",
        "            _ , predicted = torch.max(outputs.data , 1)\n",
        "            total += labels.size(0)\n",
        "            correct+= (predicted == labels).sum()\n",
        "            accuracy = (correct/total)*100\n",
        "            print('Loss:{},Accuracy: {}'.format(loss.item(), accuracy))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vImtO12E4bwl",
        "outputId": "b3026b4b-a2aa-4400-f7ec-a4fc67b7abe2"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:1.5844229459762573,Accuracy: 84.375\n",
            "Loss:1.5844229459762573,Accuracy: 84.375\n",
            "Loss:1.5844229459762573,Accuracy: 88.54167175292969\n",
            "Loss:1.5844229459762573,Accuracy: 86.71875\n",
            "Loss:1.5844229459762573,Accuracy: 86.25\n",
            "Loss:1.5844229459762573,Accuracy: 86.71875\n",
            "Loss:1.5844229459762573,Accuracy: 87.27678680419922\n",
            "Loss:1.5844229459762573,Accuracy: 87.3046875\n",
            "Loss:1.5844229459762573,Accuracy: 87.84722137451172\n",
            "Loss:1.5844229459762573,Accuracy: 87.5\n",
            "Loss:1.5844229459762573,Accuracy: 87.35795593261719\n",
            "Loss:1.5844229459762573,Accuracy: 86.97917175292969\n",
            "Loss:1.5844229459762573,Accuracy: 87.37980651855469\n",
            "Loss:1.5844229459762573,Accuracy: 87.61161041259766\n",
            "Loss:1.5844229459762573,Accuracy: 87.70833587646484\n",
            "Loss:1.5844229459762573,Accuracy: 87.3046875\n",
            "Loss:1.5844229459762573,Accuracy: 87.5\n",
            "Loss:1.5844229459762573,Accuracy: 87.41319274902344\n",
            "Loss:1.5844229459762573,Accuracy: 87.33552551269531\n",
            "Loss:1.5844229459762573,Accuracy: 87.65625\n",
            "Loss:1.5844229459762573,Accuracy: 87.27678680419922\n",
            "Loss:1.5844229459762573,Accuracy: 87.4289779663086\n",
            "Loss:1.5844229459762573,Accuracy: 87.22826385498047\n",
            "Loss:1.5844229459762573,Accuracy: 87.23957824707031\n",
            "Loss:1.5844229459762573,Accuracy: 86.5625\n",
            "Loss:1.5844229459762573,Accuracy: 86.53846740722656\n",
            "Loss:1.5844229459762573,Accuracy: 86.68981170654297\n",
            "Loss:1.5844229459762573,Accuracy: 86.60713958740234\n",
            "Loss:1.5844229459762573,Accuracy: 86.42241668701172\n",
            "Loss:1.5844229459762573,Accuracy: 86.45832824707031\n",
            "Loss:1.5844229459762573,Accuracy: 86.18951416015625\n",
            "Loss:1.5844229459762573,Accuracy: 86.03515625\n",
            "Loss:1.5844229459762573,Accuracy: 85.79545593261719\n",
            "Loss:1.5844229459762573,Accuracy: 85.6617660522461\n",
            "Loss:1.5844229459762573,Accuracy: 85.80357360839844\n",
            "Loss:1.5844229459762573,Accuracy: 85.63368225097656\n",
            "Loss:1.5844229459762573,Accuracy: 85.55743408203125\n",
            "Loss:1.5844229459762573,Accuracy: 85.48519897460938\n",
            "Loss:1.5844229459762573,Accuracy: 85.49679565429688\n",
            "Loss:1.5844229459762573,Accuracy: 85.390625\n",
            "Loss:1.5844229459762573,Accuracy: 85.40396118164062\n",
            "Loss:1.5844229459762573,Accuracy: 85.60267639160156\n",
            "Loss:1.5844229459762573,Accuracy: 85.53778839111328\n",
            "Loss:1.5844229459762573,Accuracy: 85.51136016845703\n",
            "Loss:1.5844229459762573,Accuracy: 85.59027862548828\n",
            "Loss:1.5844229459762573,Accuracy: 85.46195983886719\n",
            "Loss:1.5844229459762573,Accuracy: 85.57180786132812\n",
            "Loss:1.5844229459762573,Accuracy: 85.64453125\n",
            "Loss:1.5844229459762573,Accuracy: 85.71428680419922\n",
            "Loss:1.5844229459762573,Accuracy: 85.75\n",
            "Loss:1.5844229459762573,Accuracy: 85.72303771972656\n",
            "Loss:1.5844229459762573,Accuracy: 85.81730651855469\n",
            "Loss:1.5844229459762573,Accuracy: 85.79009246826172\n",
            "Loss:1.5844229459762573,Accuracy: 85.82176208496094\n",
            "Loss:1.5844229459762573,Accuracy: 85.88067626953125\n",
            "Loss:1.5844229459762573,Accuracy: 85.9375\n",
            "Loss:1.5844229459762573,Accuracy: 86.01973724365234\n",
            "Loss:1.5844229459762573,Accuracy: 86.04525756835938\n",
            "Loss:1.5844229459762573,Accuracy: 86.0169448852539\n",
            "Loss:1.5844229459762573,Accuracy: 85.88541412353516\n",
            "Loss:1.5844229459762573,Accuracy: 85.73258209228516\n",
            "Loss:1.5844229459762573,Accuracy: 85.8366928100586\n",
            "Loss:1.5844229459762573,Accuracy: 85.86309814453125\n",
            "Loss:1.5844229459762573,Accuracy: 85.888671875\n",
            "Loss:1.5844229459762573,Accuracy: 85.91345977783203\n",
            "Loss:1.5844229459762573,Accuracy: 85.98484802246094\n",
            "Loss:1.5844229459762573,Accuracy: 85.98413848876953\n",
            "Loss:1.5844229459762573,Accuracy: 86.02941131591797\n",
            "Loss:1.5844229459762573,Accuracy: 85.9827880859375\n",
            "Loss:1.5844229459762573,Accuracy: 86.09375\n",
            "Loss:1.5844229459762573,Accuracy: 86.00352478027344\n",
            "Loss:1.5844229459762573,Accuracy: 86.00260925292969\n",
            "Loss:1.5844229459762573,Accuracy: 86.08732604980469\n",
            "Loss:1.5844229459762573,Accuracy: 86.1064224243164\n",
            "Loss:1.5844229459762573,Accuracy: 86.1875\n",
            "Loss:1.5844229459762573,Accuracy: 86.22533416748047\n",
            "Loss:1.5844229459762573,Accuracy: 86.26216888427734\n",
            "Loss:1.5844229459762573,Accuracy: 86.17788696289062\n",
            "Loss:1.5844229459762573,Accuracy: 86.15505981445312\n",
            "Loss:1.5844229459762573,Accuracy: 86.171875\n",
            "Loss:1.5844229459762573,Accuracy: 86.13040161132812\n",
            "Loss:1.5844229459762573,Accuracy: 86.10899353027344\n",
            "Loss:1.5844229459762573,Accuracy: 86.14457702636719\n",
            "Loss:1.5844229459762573,Accuracy: 86.16071319580078\n",
            "Loss:1.5844229459762573,Accuracy: 86.08455657958984\n",
            "Loss:1.5844229459762573,Accuracy: 86.06468200683594\n",
            "Loss:1.5844229459762573,Accuracy: 86.11709594726562\n",
            "Loss:1.5844229459762573,Accuracy: 86.09729766845703\n",
            "Loss:1.5844229459762573,Accuracy: 85.97261047363281\n",
            "Loss:1.5844229459762573,Accuracy: 86.00694274902344\n",
            "Loss:1.5844229459762573,Accuracy: 85.90316009521484\n",
            "Loss:1.5844229459762573,Accuracy: 85.90353393554688\n",
            "Loss:1.5844229459762573,Accuracy: 85.90389251708984\n",
            "Loss:1.5844229459762573,Accuracy: 85.97074890136719\n",
            "Loss:1.5844229459762573,Accuracy: 85.95394134521484\n",
            "Loss:1.5844229459762573,Accuracy: 86.01887512207031\n",
            "Loss:1.5844229459762573,Accuracy: 86.06636810302734\n",
            "Loss:1.5844229459762573,Accuracy: 86.01721954345703\n",
            "Loss:1.5844229459762573,Accuracy: 86.0006332397461\n",
            "Loss:1.5844229459762573,Accuracy: 85.9375\n",
            "Loss:1.5844229459762573,Accuracy: 85.95296478271484\n",
            "Loss:1.5844229459762573,Accuracy: 85.9375\n",
            "Loss:1.5844229459762573,Accuracy: 85.9830093383789\n",
            "Loss:1.5844229459762573,Accuracy: 85.96754455566406\n",
            "Loss:1.5844229459762573,Accuracy: 85.98214721679688\n",
            "Loss:1.5844229459762573,Accuracy: 85.95223999023438\n",
            "Loss:1.5844229459762573,Accuracy: 85.92289733886719\n",
            "Loss:1.5844229459762573,Accuracy: 85.89409637451172\n",
            "Loss:1.5844229459762573,Accuracy: 85.88015747070312\n",
            "Loss:1.5844229459762573,Accuracy: 85.88067626953125\n",
            "Loss:1.5844229459762573,Accuracy: 85.86711883544922\n",
            "Loss:1.5844229459762573,Accuracy: 85.89564514160156\n",
            "Loss:1.5844229459762573,Accuracy: 85.85453796386719\n",
            "Loss:1.5844229459762573,Accuracy: 85.88267517089844\n",
            "Loss:1.5844229459762573,Accuracy: 85.91032409667969\n",
            "Loss:1.5844229459762573,Accuracy: 85.96443939208984\n",
            "Loss:1.5844229459762573,Accuracy: 85.99092102050781\n",
            "Loss:1.5844229459762573,Accuracy: 86.00370788574219\n",
            "Loss:1.5844229459762573,Accuracy: 86.0031509399414\n",
            "Loss:1.5844229459762573,Accuracy: 85.92447662353516\n",
            "Loss:1.5844229459762573,Accuracy: 85.9375\n",
            "Loss:1.5844229459762573,Accuracy: 85.97592163085938\n",
            "Loss:1.5844229459762573,Accuracy: 85.9375\n",
            "Loss:1.5844229459762573,Accuracy: 85.95010375976562\n",
            "Loss:1.5844229459762573,Accuracy: 85.9375\n",
            "Loss:1.5844229459762573,Accuracy: 85.87549591064453\n",
            "Loss:1.5844229459762573,Accuracy: 85.8882827758789\n",
            "Loss:1.5844229459762573,Accuracy: 85.87646484375\n",
            "Loss:1.5844229459762573,Accuracy: 85.87693786621094\n",
            "Loss:1.5844229459762573,Accuracy: 85.86538696289062\n",
            "Loss:1.5844229459762573,Accuracy: 85.90171813964844\n",
            "Loss:1.5844229459762573,Accuracy: 85.87831115722656\n",
            "Loss:1.5844229459762573,Accuracy: 85.85526275634766\n",
            "Loss:1.5844229459762573,Accuracy: 85.85588073730469\n",
            "Loss:1.5844229459762573,Accuracy: 85.83333587646484\n",
            "Loss:1.5844229459762573,Accuracy: 85.79963684082031\n",
            "Loss:1.5844229459762573,Accuracy: 85.82344818115234\n",
            "Loss:1.5844229459762573,Accuracy: 85.85824584960938\n",
            "Loss:1.5844229459762573,Accuracy: 85.82508850097656\n",
            "Loss:1.5844229459762573,Accuracy: 85.81473541259766\n",
            "Loss:1.5844229459762573,Accuracy: 85.78236389160156\n",
            "Loss:1.5844229459762573,Accuracy: 85.7944564819336\n",
            "Loss:1.5844229459762573,Accuracy: 85.7408218383789\n",
            "Loss:1.5844229459762573,Accuracy: 85.7204818725586\n",
            "Loss:1.5844229459762573,Accuracy: 85.73275756835938\n",
            "Loss:1.5844229459762573,Accuracy: 85.71275329589844\n",
            "Loss:1.5844229459762573,Accuracy: 85.71428680419922\n",
            "Loss:1.5844229459762573,Accuracy: 85.6946792602539\n",
            "Loss:1.5844229459762573,Accuracy: 85.72776794433594\n",
            "Loss:1.5844229459762573,Accuracy: 85.67707824707031\n",
            "Loss:1.5844229459762573,Accuracy: 85.69950103759766\n",
            "Loss:1.5844229459762573,Accuracy: 85.68051147460938\n",
            "Loss:1.5844229459762573,Accuracy: 85.68218994140625\n",
            "Loss:1.5844229459762573,Accuracy: 85.67369842529297\n",
            "Loss:1.5844229459762573,Accuracy: 85.68548583984375\n",
            "Loss:1.5844229459762573,Accuracy: 85.71714782714844\n",
            "Loss:1.5844229459762573,Accuracy: 85.73999786376953\n",
            "Loss:1.5932106971740723,Accuracy: 89.0625\n",
            "Loss:1.5932106971740723,Accuracy: 89.84375\n",
            "Loss:1.5932106971740723,Accuracy: 90.10417175292969\n",
            "Loss:1.5932106971740723,Accuracy: 89.453125\n",
            "Loss:1.5932106971740723,Accuracy: 86.5625\n",
            "Loss:1.5932106971740723,Accuracy: 86.97917175292969\n",
            "Loss:1.5932106971740723,Accuracy: 87.5\n",
            "Loss:1.5932106971740723,Accuracy: 87.6953125\n",
            "Loss:1.5932106971740723,Accuracy: 87.3263931274414\n",
            "Loss:1.5932106971740723,Accuracy: 87.5\n",
            "Loss:1.5932106971740723,Accuracy: 87.5\n",
            "Loss:1.5932106971740723,Accuracy: 87.63020324707031\n",
            "Loss:1.5932106971740723,Accuracy: 87.86058044433594\n",
            "Loss:1.5932106971740723,Accuracy: 87.83482360839844\n",
            "Loss:1.5932106971740723,Accuracy: 87.8125\n",
            "Loss:1.5932106971740723,Accuracy: 87.5\n",
            "Loss:1.5932106971740723,Accuracy: 87.13235473632812\n",
            "Loss:1.5932106971740723,Accuracy: 87.15277862548828\n",
            "Loss:1.5932106971740723,Accuracy: 87.25328826904297\n",
            "Loss:1.5932106971740723,Accuracy: 87.265625\n",
            "Loss:1.5932106971740723,Accuracy: 87.20238494873047\n",
            "Loss:1.5932106971740723,Accuracy: 87.35795593261719\n",
            "Loss:1.5932106971740723,Accuracy: 87.02445983886719\n",
            "Loss:1.5932106971740723,Accuracy: 87.109375\n",
            "Loss:1.5932106971740723,Accuracy: 87.125\n",
            "Loss:1.5932106971740723,Accuracy: 87.37980651855469\n",
            "Loss:1.5932106971740723,Accuracy: 87.15277862548828\n",
            "Loss:1.5932106971740723,Accuracy: 87.16517639160156\n",
            "Loss:1.5932106971740723,Accuracy: 87.01508331298828\n",
            "Loss:1.5932106971740723,Accuracy: 86.92708587646484\n",
            "Loss:1.5932106971740723,Accuracy: 86.74394989013672\n",
            "Loss:1.5932106971740723,Accuracy: 86.81640625\n",
            "Loss:1.5932106971740723,Accuracy: 86.60037994384766\n",
            "Loss:1.5932106971740723,Accuracy: 86.58088684082031\n",
            "Loss:1.5932106971740723,Accuracy: 86.42857360839844\n",
            "Loss:1.5932106971740723,Accuracy: 86.37152862548828\n",
            "Loss:1.5932106971740723,Accuracy: 86.48648834228516\n",
            "Loss:1.5932106971740723,Accuracy: 86.55427551269531\n",
            "Loss:1.5932106971740723,Accuracy: 86.73878479003906\n",
            "Loss:1.5932106971740723,Accuracy: 86.6015625\n",
            "Loss:1.5932106971740723,Accuracy: 86.66158294677734\n",
            "Loss:1.5932106971740723,Accuracy: 86.71875\n",
            "Loss:1.5932106971740723,Accuracy: 86.88227081298828\n",
            "Loss:1.5932106971740723,Accuracy: 86.89630889892578\n",
            "Loss:1.5932106971740723,Accuracy: 86.90972137451172\n",
            "Loss:1.5932106971740723,Accuracy: 86.75271606445312\n",
            "Loss:1.5932106971740723,Accuracy: 86.76861572265625\n",
            "Loss:1.5932106971740723,Accuracy: 86.71875\n",
            "Loss:1.5932106971740723,Accuracy: 86.57525634765625\n",
            "Loss:1.5932106971740723,Accuracy: 86.59375\n",
            "Loss:1.5932106971740723,Accuracy: 86.58088684082031\n",
            "Loss:1.5932106971740723,Accuracy: 86.65865325927734\n",
            "Loss:1.5932106971740723,Accuracy: 86.67453002929688\n",
            "Loss:1.5932106971740723,Accuracy: 86.60301208496094\n",
            "Loss:1.5932106971740723,Accuracy: 86.61931610107422\n",
            "Loss:1.5932106971740723,Accuracy: 86.57923889160156\n",
            "Loss:1.5932106971740723,Accuracy: 86.56797790527344\n",
            "Loss:1.5932106971740723,Accuracy: 86.5301742553711\n",
            "Loss:1.5932106971740723,Accuracy: 86.5730972290039\n",
            "Loss:1.5932106971740723,Accuracy: 86.58854675292969\n",
            "Loss:1.5932106971740723,Accuracy: 86.57786560058594\n",
            "Loss:1.5932106971740723,Accuracy: 86.66835021972656\n",
            "Loss:1.5932106971740723,Accuracy: 86.60713958740234\n",
            "Loss:1.5932106971740723,Accuracy: 86.6455078125\n",
            "Loss:1.5932106971740723,Accuracy: 86.5625\n",
            "Loss:1.5932106971740723,Accuracy: 86.60037994384766\n",
            "Loss:1.5932106971740723,Accuracy: 86.68376922607422\n",
            "Loss:1.5932106971740723,Accuracy: 86.71875\n",
            "Loss:1.5932106971740723,Accuracy: 86.75271606445312\n",
            "Loss:1.5932106971740723,Accuracy: 86.85267639160156\n",
            "Loss:1.5932106971740723,Accuracy: 86.81777954101562\n",
            "Loss:1.5932106971740723,Accuracy: 86.76215362548828\n",
            "Loss:1.5932106971740723,Accuracy: 86.70804595947266\n",
            "Loss:1.5932106971740723,Accuracy: 86.71875\n",
            "Loss:1.5932106971740723,Accuracy: 86.64583587646484\n",
            "Loss:1.5932106971740723,Accuracy: 86.63651275634766\n",
            "Loss:1.5932106971740723,Accuracy: 86.7897720336914\n",
            "Loss:1.5932106971740723,Accuracy: 86.73878479003906\n",
            "Loss:1.5932106971740723,Accuracy: 86.8473129272461\n",
            "Loss:1.5932106971740723,Accuracy: 86.796875\n",
            "Loss:1.5932106971740723,Accuracy: 86.80555725097656\n",
            "Loss:1.5932106971740723,Accuracy: 86.81402587890625\n",
            "Loss:1.5932106971740723,Accuracy: 86.69050598144531\n",
            "Loss:1.5932106971740723,Accuracy: 86.73735046386719\n",
            "Loss:1.5932106971740723,Accuracy: 86.78308868408203\n",
            "Loss:1.5932106971740723,Accuracy: 86.8095932006836\n",
            "Loss:1.5932106971740723,Accuracy: 86.85344696044922\n",
            "Loss:1.5932106971740723,Accuracy: 86.9140625\n",
            "Loss:1.5932106971740723,Accuracy: 86.78019714355469\n",
            "Loss:1.5932106971740723,Accuracy: 86.77083587646484\n",
            "Loss:1.5932106971740723,Accuracy: 86.7445068359375\n",
            "Loss:1.5932106971740723,Accuracy: 86.78668975830078\n",
            "Loss:1.5932106971740723,Accuracy: 86.79434967041016\n",
            "Loss:1.5932106971740723,Accuracy: 86.7353744506836\n",
            "Loss:1.5932106971740723,Accuracy: 86.74342346191406\n",
            "Loss:1.5932106971740723,Accuracy: 86.68620300292969\n",
            "Loss:1.5932106971740723,Accuracy: 86.66236877441406\n",
            "Loss:1.5932106971740723,Accuracy: 86.68685913085938\n",
            "Loss:1.5932106971740723,Accuracy: 86.69507598876953\n",
            "Loss:1.5932106971740723,Accuracy: 86.75\n",
            "Loss:1.5932106971740723,Accuracy: 86.78836822509766\n",
            "Loss:1.5932106971740723,Accuracy: 86.79534149169922\n",
            "Loss:1.5932106971740723,Accuracy: 86.80218505859375\n",
            "Loss:1.5932106971740723,Accuracy: 86.82391357421875\n",
            "Loss:1.5932106971740723,Accuracy: 86.86012268066406\n",
            "Loss:1.5932106971740723,Accuracy: 86.836669921875\n",
            "Loss:1.5932106971740723,Accuracy: 86.90128326416016\n",
            "Loss:1.5932106971740723,Accuracy: 86.9646987915039\n",
            "Loss:1.5932106971740723,Accuracy: 86.98394012451172\n",
            "Loss:1.5932106971740723,Accuracy: 86.97442626953125\n",
            "Loss:1.5932106971740723,Accuracy: 86.95101165771484\n",
            "Loss:1.5932106971740723,Accuracy: 86.99776458740234\n",
            "Loss:1.5932106971740723,Accuracy: 86.96073150634766\n",
            "Loss:1.5932106971740723,Accuracy: 87.0202865600586\n",
            "Loss:1.5932106971740723,Accuracy: 87.06521606445312\n",
            "Loss:1.5932106971740723,Accuracy: 87.01508331298828\n",
            "Loss:1.5932106971740723,Accuracy: 87.05929565429688\n",
            "Loss:1.5932106971740723,Accuracy: 87.02330780029297\n",
            "Loss:1.5932106971740723,Accuracy: 87.00105285644531\n",
            "Loss:1.5932106971740723,Accuracy: 87.04426574707031\n",
            "Loss:1.5932106971740723,Accuracy: 87.07386016845703\n",
            "Loss:1.5932106971740723,Accuracy: 86.98770141601562\n",
            "Loss:1.5932106971740723,Accuracy: 86.9537582397461\n",
            "Loss:1.5932106971740723,Accuracy: 86.94556427001953\n",
            "Loss:1.5932106971740723,Accuracy: 86.9124984741211\n",
            "Loss:1.5932106971740723,Accuracy: 86.9047622680664\n",
            "Loss:1.5932106971740723,Accuracy: 86.87254333496094\n",
            "Loss:1.5932106971740723,Accuracy: 86.87744140625\n",
            "Loss:1.5932106971740723,Accuracy: 86.88227081298828\n",
            "Loss:1.5932106971740723,Accuracy: 86.82691955566406\n",
            "Loss:1.5932106971740723,Accuracy: 86.85591888427734\n",
            "Loss:1.5932106971740723,Accuracy: 86.93181610107422\n",
            "Loss:1.5932106971740723,Accuracy: 86.95958709716797\n",
            "Loss:1.5932106971740723,Accuracy: 86.94029998779297\n",
            "Loss:1.5932106971740723,Accuracy: 86.93286895751953\n",
            "Loss:1.5932106971740723,Accuracy: 86.94853210449219\n",
            "Loss:1.5932106971740723,Accuracy: 86.95255279541016\n",
            "Loss:1.5932106971740723,Accuracy: 86.89990997314453\n",
            "Loss:1.5932106971740723,Accuracy: 86.92671203613281\n",
            "Loss:1.5932106971740723,Accuracy: 86.953125\n",
            "Loss:1.5932106971740723,Accuracy: 86.97917175292969\n",
            "Loss:1.5932106971740723,Accuracy: 86.98283386230469\n",
            "Loss:1.5932106971740723,Accuracy: 86.96459197998047\n",
            "Loss:1.5932106971740723,Accuracy: 86.94660949707031\n",
            "Loss:1.5932106971740723,Accuracy: 86.98275756835938\n",
            "Loss:1.5932106971740723,Accuracy: 86.9863052368164\n",
            "Loss:1.5932106971740723,Accuracy: 86.96853637695312\n",
            "Loss:1.5932106971740723,Accuracy: 86.87711334228516\n",
            "Loss:1.5932106971740723,Accuracy: 86.88129425048828\n",
            "Loss:1.5932106971740723,Accuracy: 86.90625\n",
            "Loss:1.5932106971740723,Accuracy: 86.9101791381836\n",
            "Loss:1.5932106971740723,Accuracy: 86.90377807617188\n",
            "Loss:1.5932106971740723,Accuracy: 86.85661315917969\n",
            "Loss:1.5932106971740723,Accuracy: 86.84050750732422\n",
            "Loss:1.5932106971740723,Accuracy: 86.78427124023438\n",
            "Loss:1.5932106971740723,Accuracy: 86.77884674072266\n",
            "Loss:1.5932106971740723,Accuracy: 86.79000091552734\n",
            "Loss:1.592132568359375,Accuracy: 79.6875\n",
            "Loss:1.592132568359375,Accuracy: 83.59375\n",
            "Loss:1.592132568359375,Accuracy: 85.9375\n",
            "Loss:1.592132568359375,Accuracy: 86.328125\n",
            "Loss:1.592132568359375,Accuracy: 86.25\n",
            "Loss:1.592132568359375,Accuracy: 85.9375\n",
            "Loss:1.592132568359375,Accuracy: 84.82142639160156\n",
            "Loss:1.592132568359375,Accuracy: 85.7421875\n",
            "Loss:1.592132568359375,Accuracy: 85.59027862548828\n",
            "Loss:1.592132568359375,Accuracy: 85.15625\n",
            "Loss:1.592132568359375,Accuracy: 85.79545593261719\n",
            "Loss:1.592132568359375,Accuracy: 85.546875\n",
            "Loss:1.592132568359375,Accuracy: 85.21634674072266\n",
            "Loss:1.592132568359375,Accuracy: 85.37946319580078\n",
            "Loss:1.592132568359375,Accuracy: 85.52083587646484\n",
            "Loss:1.592132568359375,Accuracy: 85.64453125\n",
            "Loss:1.592132568359375,Accuracy: 85.84558868408203\n",
            "Loss:1.592132568359375,Accuracy: 85.67707824707031\n",
            "Loss:1.592132568359375,Accuracy: 85.85526275634766\n",
            "Loss:1.592132568359375,Accuracy: 85.9375\n",
            "Loss:1.592132568359375,Accuracy: 85.9375\n",
            "Loss:1.592132568359375,Accuracy: 86.0085220336914\n",
            "Loss:1.592132568359375,Accuracy: 85.9375\n",
            "Loss:1.592132568359375,Accuracy: 86.00260925292969\n",
            "Loss:1.592132568359375,Accuracy: 85.875\n",
            "Loss:1.592132568359375,Accuracy: 85.9375\n",
            "Loss:1.592132568359375,Accuracy: 86.1111068725586\n",
            "Loss:1.592132568359375,Accuracy: 86.27232360839844\n",
            "Loss:1.592132568359375,Accuracy: 86.58405303955078\n",
            "Loss:1.592132568359375,Accuracy: 86.61458587646484\n",
            "Loss:1.592132568359375,Accuracy: 86.64315032958984\n",
            "Loss:1.592132568359375,Accuracy: 86.669921875\n",
            "Loss:1.592132568359375,Accuracy: 86.74242401123047\n",
            "Loss:1.592132568359375,Accuracy: 86.67279815673828\n",
            "Loss:1.592132568359375,Accuracy: 86.65178680419922\n",
            "Loss:1.592132568359375,Accuracy: 86.67534637451172\n",
            "Loss:1.592132568359375,Accuracy: 86.31756591796875\n",
            "Loss:1.592132568359375,Accuracy: 86.43091583251953\n",
            "Loss:1.592132568359375,Accuracy: 86.45832824707031\n",
            "Loss:1.592132568359375,Accuracy: 86.6796875\n",
            "Loss:1.592132568359375,Accuracy: 86.92835235595703\n",
            "Loss:1.592132568359375,Accuracy: 86.94196319580078\n",
            "Loss:1.592132568359375,Accuracy: 86.95494079589844\n",
            "Loss:1.592132568359375,Accuracy: 86.89630889892578\n",
            "Loss:1.592132568359375,Accuracy: 86.77083587646484\n",
            "Loss:1.592132568359375,Accuracy: 86.65081024169922\n",
            "Loss:1.592132568359375,Accuracy: 86.6023941040039\n",
            "Loss:1.592132568359375,Accuracy: 86.65364074707031\n",
            "Loss:1.592132568359375,Accuracy: 86.63903045654297\n",
            "Loss:1.592132568359375,Accuracy: 86.65625\n",
            "Loss:1.592132568359375,Accuracy: 86.55024719238281\n",
            "Loss:1.592132568359375,Accuracy: 86.68870544433594\n",
            "Loss:1.592132568359375,Accuracy: 86.61557006835938\n",
            "Loss:1.592132568359375,Accuracy: 86.66088104248047\n",
            "Loss:1.592132568359375,Accuracy: 86.6477279663086\n",
            "Loss:1.592132568359375,Accuracy: 86.63504791259766\n",
            "Loss:1.592132568359375,Accuracy: 86.51315307617188\n",
            "Loss:1.592132568359375,Accuracy: 86.61099243164062\n",
            "Loss:1.592132568359375,Accuracy: 86.67902374267578\n",
            "Loss:1.592132568359375,Accuracy: 86.71875\n",
            "Loss:1.592132568359375,Accuracy: 86.60348510742188\n",
            "Loss:1.592132568359375,Accuracy: 86.51713562011719\n",
            "Loss:1.592132568359375,Accuracy: 86.60713958740234\n",
            "Loss:1.592132568359375,Accuracy: 86.572265625\n",
            "Loss:1.592132568359375,Accuracy: 86.61058044433594\n",
            "Loss:1.592132568359375,Accuracy: 86.67140197753906\n",
            "Loss:1.592132568359375,Accuracy: 86.73040771484375\n",
            "Loss:1.592132568359375,Accuracy: 86.81066131591797\n",
            "Loss:1.592132568359375,Accuracy: 86.79800415039062\n",
            "Loss:1.592132568359375,Accuracy: 86.67410278320312\n",
            "Loss:1.592132568359375,Accuracy: 86.75176239013672\n",
            "Loss:1.592132568359375,Accuracy: 86.71875\n",
            "Loss:1.592132568359375,Accuracy: 86.66523742675781\n",
            "Loss:1.592132568359375,Accuracy: 86.71875\n",
            "Loss:1.592132568359375,Accuracy: 86.66666412353516\n",
            "Loss:1.592132568359375,Accuracy: 86.59539031982422\n",
            "Loss:1.592132568359375,Accuracy: 86.58685302734375\n",
            "Loss:1.592132568359375,Accuracy: 86.5584945678711\n",
            "Loss:1.592132568359375,Accuracy: 86.64952850341797\n",
            "Loss:1.592132568359375,Accuracy: 86.66015625\n",
            "Loss:1.592132568359375,Accuracy: 86.68981170654297\n",
            "Loss:1.592132568359375,Accuracy: 86.66158294677734\n",
            "Loss:1.592132568359375,Accuracy: 86.72816467285156\n",
            "Loss:1.592132568359375,Accuracy: 86.77455139160156\n",
            "Loss:1.592132568359375,Accuracy: 86.8382339477539\n",
            "Loss:1.592132568359375,Accuracy: 86.8095932006836\n",
            "Loss:1.592132568359375,Accuracy: 86.79956817626953\n",
            "Loss:1.592132568359375,Accuracy: 86.82527923583984\n",
            "Loss:1.592132568359375,Accuracy: 86.88553619384766\n",
            "Loss:1.592132568359375,Accuracy: 86.94444274902344\n",
            "Loss:1.592132568359375,Accuracy: 86.88186645507812\n",
            "Loss:1.592132568359375,Accuracy: 86.78668975830078\n",
            "Loss:1.592132568359375,Accuracy: 86.8111572265625\n",
            "Loss:1.592132568359375,Accuracy: 86.8351058959961\n",
            "Loss:1.592132568359375,Accuracy: 86.84210968017578\n",
            "Loss:1.592132568359375,Accuracy: 86.962890625\n",
            "Loss:1.592132568359375,Accuracy: 86.95231628417969\n",
            "Loss:1.592132568359375,Accuracy: 86.84630584716797\n",
            "Loss:1.592132568359375,Accuracy: 86.82133483886719\n",
            "Loss:1.592132568359375,Accuracy: 86.84375\n",
            "Loss:1.592132568359375,Accuracy: 86.85025024414062\n",
            "Loss:1.592132568359375,Accuracy: 86.84129333496094\n",
            "Loss:1.592132568359375,Accuracy: 86.92354583740234\n",
            "Loss:1.592132568359375,Accuracy: 86.94410705566406\n",
            "Loss:1.592132568359375,Accuracy: 87.00892639160156\n",
            "Loss:1.592132568359375,Accuracy: 87.04303741455078\n",
            "Loss:1.592132568359375,Accuracy: 87.06192016601562\n",
            "Loss:1.592132568359375,Accuracy: 87.05150604248047\n",
            "Loss:1.592132568359375,Accuracy: 87.01261138916016\n",
            "Loss:1.592132568359375,Accuracy: 87.01704406738281\n",
            "Loss:1.592132568359375,Accuracy: 87.02140045166016\n",
            "Loss:1.592132568359375,Accuracy: 87.05357360839844\n",
            "Loss:1.592132568359375,Accuracy: 87.02986907958984\n",
            "Loss:1.592132568359375,Accuracy: 87.07511138916016\n",
            "Loss:1.592132568359375,Accuracy: 87.02445983886719\n",
            "Loss:1.592132568359375,Accuracy: 86.98814392089844\n",
            "Loss:1.592132568359375,Accuracy: 86.95246124267578\n",
            "Loss:1.592132568359375,Accuracy: 86.95709991455078\n",
            "Loss:1.592132568359375,Accuracy: 86.92227172851562\n",
            "Loss:1.592132568359375,Accuracy: 86.953125\n",
            "Loss:1.592132568359375,Accuracy: 86.9576416015625\n",
            "Loss:1.592132568359375,Accuracy: 86.96208953857422\n",
            "Loss:1.592132568359375,Accuracy: 86.94105529785156\n",
            "Loss:1.592132568359375,Accuracy: 86.95816040039062\n",
            "Loss:1.592132568359375,Accuracy: 86.92500305175781\n",
            "Loss:1.592132568359375,Accuracy: 86.9295654296875\n",
            "Loss:1.592132568359375,Accuracy: 86.95865631103516\n",
            "Loss:1.592132568359375,Accuracy: 86.87744140625\n",
            "Loss:1.592132568359375,Accuracy: 86.906494140625\n",
            "Loss:1.592132568359375,Accuracy: 86.95913696289062\n",
            "Loss:1.592132568359375,Accuracy: 86.9990463256836\n",
            "Loss:1.592132568359375,Accuracy: 87.0265121459961\n",
            "Loss:1.592132568359375,Accuracy: 87.04182434082031\n",
            "Loss:1.592132568359375,Accuracy: 87.04524230957031\n",
            "Loss:1.592132568359375,Accuracy: 87.03704071044922\n",
            "Loss:1.592132568359375,Accuracy: 87.01746368408203\n",
            "Loss:1.592132568359375,Accuracy: 87.02098846435547\n",
            "Loss:1.592132568359375,Accuracy: 87.00181579589844\n",
            "Loss:1.592132568359375,Accuracy: 87.0053939819336\n",
            "Loss:1.592132568359375,Accuracy: 86.97544860839844\n",
            "Loss:1.592132568359375,Accuracy: 86.95700073242188\n",
            "Loss:1.592132568359375,Accuracy: 86.88380432128906\n",
            "Loss:1.592132568359375,Accuracy: 86.87718200683594\n",
            "Loss:1.592132568359375,Accuracy: 86.8923568725586\n",
            "Loss:1.592132568359375,Accuracy: 86.9073257446289\n",
            "Loss:1.592132568359375,Accuracy: 86.95419311523438\n",
            "Loss:1.592132568359375,Accuracy: 86.94728088378906\n",
            "Loss:1.592132568359375,Accuracy: 86.92990112304688\n",
            "Loss:1.592132568359375,Accuracy: 86.90226745605469\n",
            "Loss:1.592132568359375,Accuracy: 86.85416412353516\n",
            "Loss:1.592132568359375,Accuracy: 86.82740020751953\n",
            "Loss:1.592132568359375,Accuracy: 86.85238647460938\n",
            "Loss:1.592132568359375,Accuracy: 86.84640502929688\n",
            "Loss:1.592132568359375,Accuracy: 86.84050750732422\n",
            "Loss:1.592132568359375,Accuracy: 86.875\n",
            "Loss:1.592132568359375,Accuracy: 86.85897827148438\n",
            "Loss:1.592132568359375,Accuracy: 86.83999633789062\n",
            "Loss:1.554149866104126,Accuracy: 90.625\n",
            "Loss:1.554149866104126,Accuracy: 91.40625\n",
            "Loss:1.554149866104126,Accuracy: 91.66667175292969\n",
            "Loss:1.554149866104126,Accuracy: 92.1875\n",
            "Loss:1.554149866104126,Accuracy: 90.625\n",
            "Loss:1.554149866104126,Accuracy: 89.32292175292969\n",
            "Loss:1.554149866104126,Accuracy: 88.61607360839844\n",
            "Loss:1.554149866104126,Accuracy: 88.28125\n",
            "Loss:1.554149866104126,Accuracy: 88.19444274902344\n",
            "Loss:1.554149866104126,Accuracy: 88.125\n",
            "Loss:1.554149866104126,Accuracy: 88.3522720336914\n",
            "Loss:1.554149866104126,Accuracy: 88.671875\n",
            "Loss:1.554149866104126,Accuracy: 88.46153259277344\n",
            "Loss:1.554149866104126,Accuracy: 88.05803680419922\n",
            "Loss:1.554149866104126,Accuracy: 88.02082824707031\n",
            "Loss:1.554149866104126,Accuracy: 87.79296875\n",
            "Loss:1.554149866104126,Accuracy: 87.68382263183594\n",
            "Loss:1.554149866104126,Accuracy: 87.58680725097656\n",
            "Loss:1.554149866104126,Accuracy: 87.66447448730469\n",
            "Loss:1.554149866104126,Accuracy: 87.734375\n",
            "Loss:1.554149866104126,Accuracy: 87.42559814453125\n",
            "Loss:1.554149866104126,Accuracy: 87.28693389892578\n",
            "Loss:1.554149866104126,Accuracy: 87.5\n",
            "Loss:1.554149866104126,Accuracy: 87.5\n",
            "Loss:1.554149866104126,Accuracy: 87.5\n",
            "Loss:1.554149866104126,Accuracy: 87.19952392578125\n",
            "Loss:1.554149866104126,Accuracy: 86.80555725097656\n",
            "Loss:1.554149866104126,Accuracy: 86.88616180419922\n",
            "Loss:1.554149866104126,Accuracy: 86.9073257446289\n",
            "Loss:1.554149866104126,Accuracy: 86.92708587646484\n",
            "Loss:1.554149866104126,Accuracy: 86.99596405029297\n",
            "Loss:1.554149866104126,Accuracy: 86.962890625\n",
            "Loss:1.554149866104126,Accuracy: 86.7897720336914\n",
            "Loss:1.554149866104126,Accuracy: 86.81066131591797\n",
            "Loss:1.554149866104126,Accuracy: 86.74107360839844\n",
            "Loss:1.554149866104126,Accuracy: 86.80555725097656\n",
            "Loss:1.554149866104126,Accuracy: 86.82432556152344\n",
            "Loss:1.554149866104126,Accuracy: 86.96546173095703\n",
            "Loss:1.554149866104126,Accuracy: 86.61859130859375\n",
            "Loss:1.554149866104126,Accuracy: 86.6796875\n",
            "Loss:1.554149866104126,Accuracy: 86.73780822753906\n",
            "Loss:1.554149866104126,Accuracy: 86.86756134033203\n",
            "Loss:1.554149866104126,Accuracy: 86.77326202392578\n",
            "Loss:1.554149866104126,Accuracy: 86.71875\n",
            "Loss:1.554149866104126,Accuracy: 86.77083587646484\n",
            "Loss:1.554149866104126,Accuracy: 86.78668975830078\n",
            "Loss:1.554149866104126,Accuracy: 86.66888427734375\n",
            "Loss:1.554149866104126,Accuracy: 86.65364074707031\n",
            "Loss:1.554149866104126,Accuracy: 86.63903045654297\n",
            "Loss:1.554149866104126,Accuracy: 86.65625\n",
            "Loss:1.554149866104126,Accuracy: 86.76470184326172\n",
            "Loss:1.554149866104126,Accuracy: 86.74879455566406\n",
            "Loss:1.554149866104126,Accuracy: 86.73348999023438\n",
            "Loss:1.554149866104126,Accuracy: 86.80555725097656\n",
            "Loss:1.554149866104126,Accuracy: 86.84658813476562\n",
            "Loss:1.554149866104126,Accuracy: 86.96986389160156\n",
            "Loss:1.554149866104126,Accuracy: 86.92434692382812\n",
            "Loss:1.554149866104126,Accuracy: 86.96121215820312\n",
            "Loss:1.554149866104126,Accuracy: 87.02330780029297\n",
            "Loss:1.554149866104126,Accuracy: 86.92708587646484\n",
            "Loss:1.554149866104126,Accuracy: 86.78279113769531\n",
            "Loss:1.554149866104126,Accuracy: 86.71875\n",
            "Loss:1.554149866104126,Accuracy: 86.73115539550781\n",
            "Loss:1.554149866104126,Accuracy: 86.81640625\n",
            "Loss:1.554149866104126,Accuracy: 86.875\n",
            "Loss:1.554149866104126,Accuracy: 86.76609802246094\n",
            "Loss:1.554149866104126,Accuracy: 86.75373077392578\n",
            "Loss:1.554149866104126,Accuracy: 86.81066131591797\n",
            "Loss:1.554149866104126,Accuracy: 86.88858795166016\n",
            "Loss:1.554149866104126,Accuracy: 86.85267639160156\n",
            "Loss:1.554149866104126,Accuracy: 86.83979034423828\n",
            "Loss:1.554149866104126,Accuracy: 86.9140625\n",
            "Loss:1.554149866104126,Accuracy: 86.94349670410156\n",
            "Loss:1.554149866104126,Accuracy: 86.82432556152344\n",
            "Loss:1.554149866104126,Accuracy: 86.79166412353516\n",
            "Loss:1.554149866104126,Accuracy: 86.75987243652344\n",
            "Loss:1.554149866104126,Accuracy: 86.74919128417969\n",
            "Loss:1.554149866104126,Accuracy: 86.67868041992188\n",
            "Loss:1.554149866104126,Accuracy: 86.66930389404297\n",
            "Loss:1.554149866104126,Accuracy: 86.6796875\n",
            "Loss:1.554149866104126,Accuracy: 86.7283935546875\n",
            "Loss:1.554149866104126,Accuracy: 86.77591705322266\n",
            "Loss:1.554149866104126,Accuracy: 86.70933532714844\n",
            "Loss:1.554149866104126,Accuracy: 86.79315185546875\n",
            "Loss:1.554149866104126,Accuracy: 86.78308868408203\n",
            "Loss:1.554149866104126,Accuracy: 86.84593200683594\n",
            "Loss:1.554149866104126,Accuracy: 86.88936614990234\n",
            "Loss:1.554149866104126,Accuracy: 86.86079406738281\n",
            "Loss:1.554149866104126,Accuracy: 86.8153076171875\n",
            "Loss:1.554149866104126,Accuracy: 86.78819274902344\n",
            "Loss:1.554149866104126,Accuracy: 86.79601287841797\n",
            "Loss:1.554149866104126,Accuracy: 86.85462188720703\n",
            "Loss:1.554149866104126,Accuracy: 86.9119644165039\n",
            "Loss:1.554149866104126,Accuracy: 86.91822052001953\n",
            "Loss:1.554149866104126,Accuracy: 86.97368621826172\n",
            "Loss:1.554149866104126,Accuracy: 86.962890625\n",
            "Loss:1.554149866104126,Accuracy: 86.96842956542969\n",
            "Loss:1.554149866104126,Accuracy: 87.0057373046875\n",
            "Loss:1.554149866104126,Accuracy: 87.01073455810547\n",
            "Loss:1.554149866104126,Accuracy: 87.03125\n",
            "Loss:1.554149866104126,Accuracy: 87.06682586669922\n",
            "Loss:1.554149866104126,Accuracy: 87.0557632446289\n",
            "Loss:1.554149866104126,Accuracy: 87.10558319091797\n",
            "Loss:1.554149866104126,Accuracy: 87.09434509277344\n",
            "Loss:1.554149866104126,Accuracy: 87.0238037109375\n",
            "Loss:1.554149866104126,Accuracy: 87.07252502441406\n",
            "Loss:1.554149866104126,Accuracy: 87.06192016601562\n",
            "Loss:1.554149866104126,Accuracy: 86.95023345947266\n",
            "Loss:1.554149866104126,Accuracy: 86.99827575683594\n",
            "Loss:1.554149866104126,Accuracy: 87.01704406738281\n",
            "Loss:1.554149866104126,Accuracy: 87.06362915039062\n",
            "Loss:1.554149866104126,Accuracy: 87.02567291259766\n",
            "Loss:1.554149866104126,Accuracy: 86.9883804321289\n",
            "Loss:1.554149866104126,Accuracy: 86.92434692382812\n",
            "Loss:1.554149866104126,Accuracy: 86.90216827392578\n",
            "Loss:1.554149866104126,Accuracy: 86.86691284179688\n",
            "Loss:1.554149866104126,Accuracy: 86.88568115234375\n",
            "Loss:1.554149866104126,Accuracy: 86.85116577148438\n",
            "Loss:1.554149866104126,Accuracy: 86.85661315917969\n",
            "Loss:1.554149866104126,Accuracy: 86.92708587646484\n",
            "Loss:1.554149866104126,Accuracy: 86.93181610107422\n",
            "Loss:1.554149866104126,Accuracy: 86.96208953857422\n",
            "Loss:1.554149866104126,Accuracy: 87.00457000732422\n",
            "Loss:1.554149866104126,Accuracy: 87.0085678100586\n",
            "Loss:1.554149866104126,Accuracy: 87.0250015258789\n",
            "Loss:1.554149866104126,Accuracy: 87.05357360839844\n",
            "Loss:1.554149866104126,Accuracy: 87.093994140625\n",
            "Loss:1.554149866104126,Accuracy: 87.04833984375\n",
            "Loss:1.554149866104126,Accuracy: 86.99127960205078\n",
            "Loss:1.554149866104126,Accuracy: 86.99519348144531\n",
            "Loss:1.554149866104126,Accuracy: 86.98712158203125\n",
            "Loss:1.554149866104126,Accuracy: 86.97917175292969\n",
            "Loss:1.554149866104126,Accuracy: 86.88909912109375\n",
            "Loss:1.554149866104126,Accuracy: 86.92863464355469\n",
            "Loss:1.554149866104126,Accuracy: 86.8865737915039\n",
            "Loss:1.554149866104126,Accuracy: 86.86811065673828\n",
            "Loss:1.554149866104126,Accuracy: 86.90693664550781\n",
            "Loss:1.554149866104126,Accuracy: 86.8659439086914\n",
            "Loss:1.554149866104126,Accuracy: 86.8480224609375\n",
            "Loss:1.554149866104126,Accuracy: 86.88616180419922\n",
            "Loss:1.554149866104126,Accuracy: 86.9348373413086\n",
            "Loss:1.554149866104126,Accuracy: 86.92781829833984\n",
            "Loss:1.554149866104126,Accuracy: 86.94274139404297\n",
            "Loss:1.554149866104126,Accuracy: 86.92491149902344\n",
            "Loss:1.554149866104126,Accuracy: 86.92887878417969\n",
            "Loss:1.554149866104126,Accuracy: 86.91138458251953\n",
            "Loss:1.554149866104126,Accuracy: 86.87287902832031\n",
            "Loss:1.554149866104126,Accuracy: 86.8665542602539\n",
            "Loss:1.554149866104126,Accuracy: 86.83934783935547\n",
            "Loss:1.554149866104126,Accuracy: 86.80208587646484\n",
            "Loss:1.554149866104126,Accuracy: 86.83775329589844\n",
            "Loss:1.554149866104126,Accuracy: 86.85238647460938\n",
            "Loss:1.554149866104126,Accuracy: 86.83619689941406\n",
            "Loss:1.554149866104126,Accuracy: 86.82020568847656\n",
            "Loss:1.554149866104126,Accuracy: 86.82459259033203\n",
            "Loss:1.554149866104126,Accuracy: 86.80889892578125\n",
            "Loss:1.554149866104126,Accuracy: 86.81999969482422\n"
          ]
        }
      ]
    }
  ]
}